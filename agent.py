import os
import streamlit as st
from rag import check_database_exists, embeddings
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain_core.prompts import ChatPromptTemplate
from langchain.chat_models import init_chat_model
from langchain_community.vectorstores import FAISS
from langchain.tools.retriever import create_retriever_tool
from dotenv import load_dotenv
load_dotenv(override=True)

# 5. Agentå¯¹è¯é“¾ + å·¥å…·è°ƒç”¨ï¼ˆæ ¸å¿ƒRAGï¼‰
DeepSeek_API_KEY = os.environ.get("DEEPSEEK_API_KEY")

def get_conversational_chain(tools, ques):
    llm = init_chat_model("deepseek-chat", model_provider="deepseek")
    prompt = ChatPromptTemplate.from_messages([
        (
            "system", 
            """ä½ æ˜¯AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œç¡®ä¿æä¾›æ‰€æœ‰ç»†èŠ‚ï¼Œå¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·è¯´"ç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­"ï¼Œä¸è¦æä¾›é”™è¯¯çš„ç­”æ¡ˆ"""
        ),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ])

    tool = [tools]
    agent = create_tool_calling_agent(llm, tool, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tool, verbose=True)

    response = agent_executor.invoke({"input": ques})
    print(response)
    st.write("ğŸ¤– å›ç­”: ", response['output'])

# 7. ç”¨æˆ·æé—®é€»è¾‘ï¼ˆè°ƒç”¨FAISSï¼‰
def get_answer_with_rag(user_question, contract_code):
    if not check_database_exists():
        raise Exception("FAISSæ•°æ®åº“ä¸å­˜åœ¨")
    # åŠ è½½FAISSæ•°æ®åº“
    new_db = FAISS.load_local("faiss_db", embeddings, allow_dangerous_deserialization=True)
        
    # æ„å»ºretrieverå·¥å…·
    retriever = new_db.as_retriever()
    retrieval_chain = create_retriever_tool(retriever, "pdf_extractor", "This tool is to give answer to queries from the pdf")
    
    # è°ƒç”¨å¯¹è¯é“¾
    response = get_conversational_chain(retrieval_chain, user_question, contract_code)
    return response
